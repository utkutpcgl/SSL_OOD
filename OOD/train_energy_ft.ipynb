{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run MyOtherNotebook.ipynb To import from other ipynb files.\n",
    "import torchvision\n",
    "import torch\n",
    "import copy\n",
    "from typing import OrderedDict\n",
    "\n",
    "NUMBER_OF_CLASSES = 10\n",
    "model_setting=\"BYOL\" # ImageNet_Pretrained, BYOL, DINO or BarlowTwins\n",
    "if model_setting ==\"ImageNet_Pretrained\":\n",
    "    ResNet = torchvision.models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "    number_of_input_features = ResNet.fc.in_features\n",
    "    class ResNetModel(torch.nn.Module):\n",
    "            def __init__(self, feature_extractor=ResNet):\n",
    "                out_features = ResNet.fc.in_features\n",
    "                super(ResNetModel, self).__init__()\n",
    "                # number_of_input_features is 2048 (Resnet50.fc.in_features)\n",
    "                self.feature_extractor = copy.deepcopy(feature_extractor)\n",
    "                number_of_input_features = out_features\n",
    "                self.feature_extractor.fc = torch.nn.Linear(number_of_input_features, NUMBER_OF_CLASSES)\n",
    "\n",
    "            def forward(self, input_frame):\n",
    "                output = self.feature_extractor(input_frame)\n",
    "                return output\n",
    "    ResNet = ResNetModel()\n",
    "else:\n",
    "    ResNet = torchvision.models.resnet50()\n",
    "    number_of_input_features = ResNet.fc.in_features\n",
    "    ResNet.fc = torch.nn.Identity()\n",
    "    if model_setting==\"BYOL\":\n",
    "        weight_path = \"/home/utku/Documents/repos/SSL_OOD/resnet50_byol_imagenet2012.pth.tar\"\n",
    "        state_dict = torch.load(weight_path)[\"online_backbone\"]\n",
    "        correct_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:]  # remove `module.`\n",
    "            correct_state_dict[name] = v\n",
    "        ResNet.load_state_dict(correct_state_dict)\n",
    "        ResNet.fc = torch.nn.Linear(number_of_input_features, NUMBER_OF_CLASSES)\n",
    "    elif model_setting ==\"BarlowTwins\":\n",
    "        weight_path = \"/home/utku/Documents/repos/SSL_OOD/barlowT_resnet50.pth\"\n",
    "        state_dict = torch.load(weight_path)\n",
    "        ResNet.load_state_dict(state_dict)\n",
    "        ResNet.fc = torch.nn.Linear(number_of_input_features, NUMBER_OF_CLASSES)\n",
    "        pass\n",
    "    elif model_setting ==\"DINO\":\n",
    "        weight_path = \"/home/utku/Documents/repos/SSL_OOD/dino_resnet50_pretrain.pth\"\n",
    "        state_dict = torch.load(weight_path)\n",
    "        ResNet.load_state_dict(state_dict)\n",
    "        ResNet.fc = torch.nn.Linear(number_of_input_features, NUMBER_OF_CLASSES)\n",
    "    else:\n",
    "        raise Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear layer eklenemiş SSL modellerini CIFAR-10 da fine tune ederken backbone freeze mi edeyim, etmeyeyim mi? Kaç epoch fine tune etmeliyim?\n",
    "- ImageNet pretrained modeli 100 epoch için CIFAR10 da train ederken hyp ları nasıl seçmeliyim. Makaleyle aynı olmak zorunda değil aslında."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run images_loader_300k.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the final layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Fine-tune all network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "# from models.wrn import WideResNet\n",
    "\n",
    "if __package__ is None:\n",
    "    import sys\n",
    "    from os import path\n",
    "\n",
    "    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n",
    "    from utils.validation_dataset import validation_split\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Tunes a CIFAR Classifier with OE',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('dataset', type=str, choices=['cifar10', 'cifar100'],\n",
    "                    help='Choose between CIFAR-10, CIFAR-100.')\n",
    "parser.add_argument('--model', '-m', type=str, default='allconv',\n",
    "                    choices=['allconv', 'wrn', 'densenet'], help='Choose architecture.')\n",
    "parser.add_argument('--calibration', '-c', action='store_true',\n",
    "                    help='Train a model to be used for calibration. This holds out some data for validation.')\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', '-e', type=int, default=10, help='Number of epochs to train.')\n",
    "parser.add_argument('--learning_rate', '-lr', type=float, default=0.001, help='The initial learning rate.')\n",
    "parser.add_argument('--batch_size', '-b', type=int, default=128, help='Batch size.')\n",
    "parser.add_argument('--od_batch_size', type=int, default=256, help='Batch size.')\n",
    "parser.add_argument('--test_bs', type=int, default=200)\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum.')\n",
    "parser.add_argument('--decay', '-d', type=float, default=0.0005, help='Weight decay (L2 penalty).')\n",
    "# WRN Architecture\n",
    "parser.add_argument('--layers', default=40, type=int, help='total number of layers')\n",
    "parser.add_argument('--widen-factor', default=2, type=int, help='widen factor')\n",
    "parser.add_argument('--droprate', default=0.3, type=float, help='dropout probability')\n",
    "# Checkpoints\n",
    "parser.add_argument('--save', '-s', type=str, default='./snapshots/', help='Folder to save checkpoints.')\n",
    "parser.add_argument('--load', '-l', type=str, default='./snapshots/pretrained', help='Checkpoint path to resume / test.')\n",
    "parser.add_argument('--test', '-t', action='store_true', help='Test only flag.')\n",
    "# Acceleration\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='0 = CPU.')\n",
    "parser.add_argument('--prefetch', type=int, default=4, help='Pre-fetching threads.')\n",
    "# EG specific\n",
    "parser.add_argument('--m_in', type=float, default=-25., help='margin for in-distribution; above this value will be penalized')\n",
    "parser.add_argument('--m_out', type=float, default=-7., help='margin for out-distribution; below this value will be penalized')\n",
    "parser.add_argument('--seed', type=int, default=1, help='seed for np(tinyimages80M sampling); 1|2|8|100|107')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "save_info = 'energy_ft'\n",
    "\n",
    "args.save = args.save+save_info\n",
    "if os.path.isdir(args.save) == False:\n",
    "    os.mkdir(args.save)\n",
    "state = {k: v for k, v in args._get_kwargs()}\n",
    "print(state)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# mean and standard deviation of channels of CIFAR-10 images\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "train_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(32, padding=4),\n",
    "                               trn.ToTensor(), trn.Normalize(mean, std)])\n",
    "test_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n",
    "\n",
    "train_data_in = dset.CIFAR10('../data/cifarpy', train=True, transform=train_transform)\n",
    "test_data = dset.CIFAR10('../data/cifarpy', train=False, transform=test_transform)\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "\n",
    "calib_indicator = ''\n",
    "if args.calibration:\n",
    "    train_data_in, val_data = validation_split(train_data_in, val_share=0.1)\n",
    "    calib_indicator = '_calib'\n",
    "\n",
    "\n",
    "# ood_data = TinyImages(transform=trn.Compose(\n",
    "#     [trn.ToTensor(), trn.ToPILImage(), trn.RandomCrop(32, padding=4),\n",
    "#      trn.RandomHorizontalFlip(), trn.ToTensor(), trn.Normalize(mean, std)]))\n",
    "\n",
    "\n",
    "ood_data = RImages_300K(transform=trn.Compose(\n",
    "    [trn.ToTensor(), trn.ToPILImage(), trn.RandomCrop(32, padding=4),\n",
    "     trn.RandomHorizontalFlip(), trn.ToTensor(), trn.Normalize(mean, std)]))\n",
    "\n",
    "train_loader_in = torch.utils.data.DataLoader(\n",
    "    train_data_in,\n",
    "    batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=args.prefetch, pin_memory=True)\n",
    "\n",
    "train_loader_out = torch.utils.data.DataLoader(\n",
    "    ood_data,\n",
    "    batch_size=args.od_batch_size, shuffle=False,\n",
    "    num_workers=args.prefetch, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=args.batch_size, shuffle=False,\n",
    "    num_workers=args.prefetch, pin_memory=True)\n",
    "\n",
    "# Create model\n",
    "# net = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n",
    "net = ResNet\n",
    "\n",
    "def recursion_change_bn(module):\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module.track_running_stats = 1\n",
    "        module.num_batches_tracked = 0\n",
    "    else:\n",
    "        for i, (name, module1) in enumerate(module._modules.items()):\n",
    "            module1 = recursion_change_bn(module1)\n",
    "    return module\n",
    "# Restore model\n",
    "model_found = False\n",
    "if args.load != '':\n",
    "    for i in range(1000 - 1, -1, -1):\n",
    "        \n",
    "        model_name = os.path.join(args.load, args.dataset + calib_indicator + '_' + args.model +\n",
    "                                  '_pretrained_epoch_' + str(i) + '.pt')\n",
    "        if os.path.isfile(model_name):\n",
    "            net.load_state_dict(torch.load(model_name))\n",
    "            print('Model restored! Epoch:', i)\n",
    "            model_found = True\n",
    "            break\n",
    "    if not model_found:\n",
    "        assert False, \"could not find model to restore\"\n",
    "\n",
    "if args.ngpu > 1:\n",
    "    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n",
    "\n",
    "if args.ngpu > 0:\n",
    "    net.cuda()\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "cudnn.benchmark = True  # fire on all cylinders\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    net.parameters(), state['learning_rate'], momentum=state['momentum'],\n",
    "    weight_decay=state['decay'], nesterov=True)\n",
    "\n",
    "\n",
    "def cosine_annealing(step, total_steps, lr_max, lr_min):\n",
    "    return lr_min + (lr_max - lr_min) * 0.5 * (\n",
    "            1 + np.cos(step / total_steps * np.pi))\n",
    "\n",
    "\n",
    "#NOTE # Lamba below always increases the learning rate for the entire training steps. It does not do a warm restart, why?\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: cosine_annealing(\n",
    "        step,\n",
    "        args.epochs * len(train_loader_in),\n",
    "        1,  # since lr_lambda computes multiplicative factor\n",
    "        1e-6 / args.learning_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image size settings\n",
    "import yaml\n",
    "with open(\"hyperparameters.yaml\", \"r\") as reader:\n",
    "    HYPS = yaml.safe_load(reader)\n",
    "lambda_energy = HYPS[\"lambda\"] # Energy loss scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# /////////////// Training (Energy fine-tuning) ///////////////\n",
    "\n",
    "def train():\n",
    "    net.train()  # enter train mode\n",
    "    loss_avg = 0.0\n",
    "\n",
    "    # start at a random point of the outlier dataset; this induces more randomness without obliterating locality\n",
    "    train_loader_out.dataset.offset = np.random.randint(len(train_loader_out.dataset))\n",
    "    for in_set, out_set in zip(train_loader_in, train_loader_out):\n",
    "        data = torch.cat((in_set[0], out_set[0]), 0)\n",
    "        target = in_set[1]\n",
    "\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        # forward\n",
    "        x = net(data)\n",
    "\n",
    "        # backward\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = F.cross_entropy(x[:len(in_set[0])], target)\n",
    "        # cross-entropy from softmax distribution to uniform distribution\n",
    "        Ec_out = -torch.logsumexp(x[len(in_set[0]):], dim=1)\n",
    "        Ec_in = -torch.logsumexp(x[:len(in_set[0])], dim=1)\n",
    "        loss += lambda_energy*(torch.pow(F.relu(Ec_in-args.m_in), 2).mean() + torch.pow(F.relu(args.m_out-Ec_out), 2).mean())\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # exponential moving average\n",
    "        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n",
    "    state['train_loss'] = loss_avg\n",
    "\n",
    "\n",
    "# test function\n",
    "def test():\n",
    "    net.eval()\n",
    "    loss_avg = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # forward\n",
    "            output = net(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "\n",
    "            # accuracy\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.data).sum().item()\n",
    "\n",
    "            # test loss average\n",
    "            loss_avg += float(loss.data)\n",
    "\n",
    "    state['test_loss'] = loss_avg / len(test_loader)\n",
    "    state['test_accuracy'] = correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "if args.test:\n",
    "    test()\n",
    "    print(state)\n",
    "    exit()\n",
    "\n",
    "# Make save directory\n",
    "if not os.path.exists(args.save):\n",
    "    os.makedirs(args.save)\n",
    "if not os.path.isdir(args.save):\n",
    "    raise Exception('%s is not a dir' % args.save)\n",
    "\n",
    "with open(os.path.join(args.save, args.dataset + calib_indicator + '_' + args.model + '_s' + str(args.seed) +\n",
    "                                  '_' + save_info+'_training_results.csv'), 'w') as f:\n",
    "    f.write('epoch,time(s),train_loss,test_loss,test_error(%)\\n')\n",
    "\n",
    "print('Beginning Training\\n')\n",
    "\n",
    "# Main loop\n",
    "for epoch in range(0, args.epochs):\n",
    "    state['epoch'] = epoch\n",
    "\n",
    "    begin_epoch = time.time()\n",
    "\n",
    "    train()\n",
    "    test()\n",
    " \n",
    "    # Save model\n",
    "    torch.save(net.state_dict(),\n",
    "               os.path.join(args.save, args.dataset + calib_indicator + '_' + args.model + '_s' + str(args.seed) +\n",
    "                            '_' + save_info + '_epoch_' + str(epoch) + '.pt'))\n",
    "    \n",
    "               # Let us not waste space and delete the previous model\n",
    "    prev_path = os.path.join(args.save, args.dataset + calib_indicator + '_' + args.model + '_s' + str(args.seed) +\n",
    "                             '_' + save_info + '_epoch_'+ str(epoch - 1) + '.pt')\n",
    "    if os.path.exists(prev_path): os.remove(prev_path)\n",
    "\n",
    "    # Show results\n",
    "    with open(os.path.join(args.save, args.dataset + calib_indicator + '_' + args.model + '_s' + str(args.seed) +\n",
    "                                      '_' + save_info + '_training_results.csv'), 'a') as f:\n",
    "        f.write('%03d,%05d,%0.6f,%0.5f,%0.2f\\n' % (\n",
    "            (epoch + 1),\n",
    "            time.time() - begin_epoch,\n",
    "            state['train_loss'],\n",
    "            state['test_loss'],\n",
    "            100 - 100. * state['test_accuracy'],\n",
    "        ))\n",
    "\n",
    "    # # print state with rounded decimals\n",
    "    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n",
    "\n",
    "    print('Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}'.format(\n",
    "        (epoch + 1),\n",
    "        int(time.time() - begin_epoch),\n",
    "        state['train_loss'],\n",
    "        state['test_loss'],\n",
    "        100 - 100. * state['test_accuracy'])\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1484ded363958018b63f22e3cca0a4db1032bdb5172b1a076544307dcdefb374"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
